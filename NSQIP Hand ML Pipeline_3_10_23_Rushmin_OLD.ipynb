{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 15:51:41.454816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Loading the key packages\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import sklearn\n",
    "import imblearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.calibration as cal\n",
    "import xgboost as xgb\n",
    "import sklearn.utils.class_weight as wt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import skopt\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import models, layers, utils, backend as K, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import tensorflow\n",
    "tensorflow.compat.v1.disable_v2_behavior() \n",
    "import random\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from numpy.ma import MaskedArray\n",
    "import sklearn.utils.fixes\n",
    "\n",
    "sklearn.utils.fixes.MaskedArray = MaskedArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.1.1\n",
      "  Using cached scikit-learn-1.1.1.tar.gz (6.8 MB)\n",
      "  Installing build dependencies ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==1.1.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Initializing dataframes to store the results\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "\n",
    "# hyperparam_results = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Procedure\": [],\n",
    "#         \"Outcome\": [],\n",
    "#         \"Features\": [],\n",
    "#         \"Model\": [],\n",
    "#         \"Rep\": [],\n",
    "#         \"Hyp_Name\": [],\n",
    "#         \"Hyp_Value\": []\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "feature_results = []\n",
    "\n",
    "auc_plot_results = []\n",
    "\n",
    "\n",
    "# Areas which below pipeline will iterate through -- CHANGE THIS BASED ON NEED\n",
    "\n",
    "\n",
    "#binary_outcomes = np.array([\"readmission_binary\", \"major_med_comp\", \"extended_los\", \"returnor\", \"wnd_related_comp\"])\n",
    "#binary_outcomes = np.array([\"readmission_binary\"])\n",
    "#binary_outcomes = np.array([\"returnor\", \"any_med_comp_no_wnd\", \"any_med_comp_wnd\", \"any_comp_overall\"])\n",
    "binary_outcomes = np.array([\"Post_lam_syndrome\"])\n",
    "\n",
    "#continuous_outcomes = np.array([\"outpatient_resource_sum\", \"los_days\"])\n",
    "#continuous_outcomes = np.array(['los_days'])\n",
    "\n",
    "continuous_outcomes = np.array([])\n",
    "\n",
    "\n",
    "outcomes = np.concatenate((binary_outcomes, continuous_outcomes))\n",
    "\n",
    "#procedures = [\"Microdisc\", \"Foraminotomy\", \"Laminectomy\", \"AXDLIF\", \"PLF\", \"PTLIF\"]\n",
    "procedures = [\"PLS\"]\n",
    "\n",
    "\n",
    "#features = [\"Spine_Institution\", \"Spine_NSQIP\", \"Peripheral_NSQIP\"]\n",
    "#features = [\"Spine_Institution\"]\n",
    "#features = [\"Peripheral_NSQIP\"]\n",
    "features = [\"PLS_features\"]\n",
    "\n",
    "model_set = [\"RF\", \"ENet\", \"XGBoost\", \"NN\", \"ASA\", \"Dummy\"]\n",
    "#model_set = [\"RF\"]\n",
    "\n",
    "#model_set = [\"XGBoost\"]\n",
    "#model_set = [\"Dummy\"]\n",
    "\n",
    "\n",
    "\n",
    "repetitions = list(range(5))\n",
    "date = \"3_11_23\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seed(rep):\n",
    "    \n",
    "    tensorflow.keras.backend.clear_session()\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(rep)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    tensorflow.random.set_seed(rep)\n",
    "    np.random.seed(rep)\n",
    "    random.seed(rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(procedure):\n",
    "    if procedure == \"PLS\": \n",
    "        readdata = pd.read_csv(r\"081524_Spine_reports_vf.csv\", sep=',')\n",
    "        \n",
    "    return readdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_columns(feature):\n",
    "    if feature == 'Peripheral_NSQIP':\n",
    "                                \n",
    "                df = data[[\n",
    "\n",
    "                    \"caseid\",\n",
    "                    \"age\", \"optime\", \"sex_recode\", \"race\" , \"ethnicity_hispanic\", \"smoke\", \"surgspec\",\n",
    "                    \"bmi\", \"diabetes\", \"dyspnea\", \"fnstatus2\", \"inout\", \"ascites\", \"hxchf\",\n",
    "                    \"hypermed\", \"renafail\", \"hxcopd\", \"dialysis\", \"wndinf\", \"steroid\", \"wtloss\", \n",
    "                    \"bleeddis\", \"transfus\", \"asaclas\", \"ventilat\", \"prsepis\", \"discancr\",\n",
    "                    \"transt\", \"wndclas\", \"htooday\", \"anesthes\",\n",
    "\n",
    "\n",
    "                    # Outcome\n",
    "                    outcome\n",
    "                ]].copy()\n",
    "\n",
    "                dum_cols = [\"race\", \"surgspec\", \"fnstatus2\", \"asaclas\", \"wndclas\", \"anesthes\", \"transt\"]\n",
    "\n",
    "                numeric_features = [\"age\", \"bmi\", \"optime\", \"htooday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for feature + outcome combo\n",
    "def full_feature_outcome_test(feature, outcome):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46.0\n"
     ]
    }
   ],
   "source": [
    "#pip install shap==0.41.0 --user\n",
    "print(shap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/92/tt9zk05n4qd65slhpygj9v4r0000gr/T/ipykernel_34197/2913844004.py:3: DtypeWarning: Columns (73,74,75,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,455,456,460,462,464,466,468,470,471,474,476,478,480,481,482,490,492,493,494,495,496,497,498,499,500,502,504,506,508,510,511,512,516,518,520,522,523,524,526,528,529,531,532,534,537,538,539,540,541,542,544,545,546,547,548,550,551,552,553,554,555,557,560,561,562,563,564,566,568,570,571,574,575,576,577,583,585,586,587,588,589,590,591,592,593,596,600,601,605,606,607,609,610,612,613,614,616,618,619,620,622,624,625,626,628,629,634,635,636,637,638,642,643,646,650,652,653,654,658,660,662,664,666,668,671,673,675,677,679,681,682,683,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,708,709,710,711,714,716,717,719,720,722,725,728,729,732,736,737,740,743,744,746,747,750,751,754,766,767,768,769,772,773,782,783,784,785,786,787,788,789,792,793,794,795,796,797,814,815,822,823,830,831,836,837,838,839,842,843,844,845,848,849,850,851,852,853) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  readdata = pd.read_csv(r\"081524_Spine_reports_vf.csv\", sep=',')\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "data = load_data(\"PLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature_outcome_test(feature=\"PLS_features\", outcome=\"Post_lam_syndrome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for procedure in procedures:\n",
    "    \n",
    "    if procedure == \"LFS\": \n",
    "        data = pd.read_csv(r\"081524_Spine_reports_vf.csv\", sep=',')\n",
    "        \n",
    "    for feature in features:\n",
    "        \n",
    "        for outcome in outcomes:\n",
    "            \n",
    "            if procedure == \"thumb_arthro\" and outcome == \"returnor\":\n",
    "                print(\"skip\")\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            if procedure == \"thumb_arthro\" and outcome == \"any_med_comp_no_wnd\":\n",
    "                print(\"skip\")\n",
    "                continue\n",
    "                \n",
    "\n",
    "            \n",
    "            individual_features_shap_full = []\n",
    "            \n",
    "            shap_file_name = procedure+\"_\"+outcome+\"_\"+\"shap_values.csv\"\n",
    "    \n",
    "\n",
    "            if feature == 'Peripheral_NSQIP':\n",
    "                                \n",
    "                df = data[[\n",
    "\n",
    "                    \"caseid\",\n",
    "                    \"age\", \"optime\", \"sex_recode\", \"race\" , \"ethnicity_hispanic\", \"smoke\", \"surgspec\",\n",
    "                    \"bmi\", \"diabetes\", \"dyspnea\", \"fnstatus2\", \"inout\", \"ascites\", \"hxchf\",\n",
    "                    \"hypermed\", \"renafail\", \"hxcopd\", \"dialysis\", \"wndinf\", \"steroid\", \"wtloss\", \n",
    "                    \"bleeddis\", \"transfus\", \"asaclas\", \"ventilat\", \"prsepis\", \"discancr\",\n",
    "                    \"transt\", \"wndclas\", \"htooday\", \"anesthes\",\n",
    "\n",
    "\n",
    "                    # Outcome\n",
    "                    outcome\n",
    "                ]].copy()\n",
    "\n",
    "                dum_cols = [\"race\", \"surgspec\", \"fnstatus2\", \"asaclas\", \"wndclas\", \"anesthes\", \"transt\"]\n",
    "\n",
    "                numeric_features = [\"age\", \"bmi\", \"optime\", \"htooday\"]\n",
    "                \n",
    "                    \n",
    "            df = df.dropna().reset_index(drop=True) # drop missing variables\n",
    "\n",
    "            n = len(df)\n",
    "            \n",
    "            y = df[outcome].values\n",
    "\n",
    "            label_n = sum(y)\n",
    "\n",
    "\n",
    "            df_preds = df.drop(columns = [outcome])\n",
    "\n",
    "            dum_df = pd.get_dummies(df_preds, columns= dum_cols)\n",
    "                \n",
    "            for rep in repetitions:\n",
    "                                \n",
    "                # Formulating the preprocessor for scaling\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "                preprocessor = ColumnTransformer([('minmax', scaler, numeric_features)], remainder = 'passthrough')\n",
    "\n",
    "                if outcome in binary_outcomes:\n",
    "                    \n",
    "                    skf = StratifiedKFold(n_splits=5, shuffle = True, random_state = rep)\n",
    "\n",
    "                elif outcome in continuous_outcomes:\n",
    "                    \n",
    "                    print(\"In Progress\")\n",
    "\n",
    "                        \n",
    "                \n",
    "                for i, (train_index, test_index) in enumerate(skf.split(dum_df, y)):\n",
    "                    \n",
    "                    # Split into train and test\n",
    "                    \n",
    "                    x_train_split = dum_df.iloc[train_index].copy()\n",
    "                    x_test_split = dum_df.iloc[test_index].copy()\n",
    "                    y_train_split = y[train_index].copy()\n",
    "                    y_test_split = y[test_index].copy()\n",
    "                    \n",
    "                    \n",
    "                    # Peel off validation set\n",
    "        \n",
    "                    x_train_actual_split, x_val_split, y_train_actual_split, y_val_split = train_test_split(x_train_split, y_train_split, stratify = y_train_split, test_size = 0.1, random_state = rep) # random process\n",
    "\n",
    "\n",
    "                    # Remove mrn and surgery start datetime and include that as the actual train and test\n",
    "                        \n",
    "                        \n",
    "                    if feature == \"Peripheral_NSQIP\":\n",
    "                        \n",
    "                        print(\"Test\")\n",
    "                \n",
    "                        x_train_actual = x_train_actual_split.drop(columns = [\"caseid\"])\n",
    "                        x_val = x_val_split.drop(columns = [\"caseid\"])\n",
    "                        y_train_actual = y_train_actual_split.copy()\n",
    "                        y_val = y_val_split.copy()\n",
    "                    \n",
    "                        x_train = x_train_split.drop(columns = [\"caseid\"])\n",
    "                        x_test = x_test_split.drop(columns = [\"caseid\"])\n",
    "                        y_train = y_train_split.copy()\n",
    "                        y_test = y_test_split.copy()\n",
    "                        \n",
    "                        x_train_std = preprocessor.fit_transform(x_train)\n",
    "\n",
    "                        x_test_std = preprocessor.transform(x_test)\n",
    "                \n",
    "                \n",
    "            \n",
    "                        # Keep a version that has those values as index columns \n",
    "                    \n",
    "                        x_test_features = x_test_split.copy()\n",
    "                        x_test_features[\"Patient_ID\"]=  np.arange(len(x_test_features))\n",
    "\n",
    "                        x_test_features_long = pd.melt(x_test_features, id_vars = [\"caseid\", \"Patient_ID\"], var_name = \"Feature_Name\", value_name = \"Feature_Actual_Value\" )\n",
    "\n",
    "\n",
    "                        x_cols = x_test.columns\n",
    "\n",
    "                        n_features = len(x_cols)\n",
    "\n",
    "\n",
    "            \n",
    "                    for model in model_set:\n",
    "\n",
    "\n",
    "                        print(procedure)\n",
    "                        print(feature)\n",
    "                        print(outcome)\n",
    "                        print(rep)\n",
    "                        print(i)\n",
    "                        print(model)\n",
    "                        print(time.strftime('%X %x %Z'))\n",
    "\n",
    "\n",
    "                        if model == \"ASA\": # note - only works for binary outcomes\n",
    "\n",
    "                            trim_train = x_train[[\"asaclas_2\", \"asaclas_3\", \"asaclas_4\"]].copy()\n",
    "                            #trim_train_reshape = trim_train.values.reshape(-1, 1)\n",
    "\n",
    "                            trim_test = x_test[[\"asaclas_2\", \"asaclas_3\", \"asaclas_4\"]].copy()\n",
    "                            #trim_test_reshape = trim_test.values.reshape(-1, 1)\n",
    "\n",
    "                            cv_model = linear_model.LogisticRegression(penalty = 'none', random_state = rep)\n",
    "                            cv_model.fit(trim_train, y_train)\n",
    "\n",
    "                            y_pred = cv_model.predict(trim_test)\n",
    "                            y_prob = cv_model.predict_proba(trim_test)\n",
    "                            y_prob_vec = y_prob[:,1]\n",
    "                            #print(y_prob_vec)\n",
    "\n",
    "\n",
    "                        elif model == \"ASA_categorical\": # note - only works for binary outcomes\n",
    "\n",
    "\n",
    "                            trim_test_categorical = x_test[\"asa_class_above_2\"]\n",
    "\n",
    "\n",
    "                            y_pred = trim_test_categorical.values.reshape(-1,1)\n",
    "                            y_prob = trim_test_categorical.values.reshape(-1,1)\n",
    "                            y_prob_vec = trim_test_categorical.values.reshape(-1,1)\n",
    "                            #print(y_prob_vec)\n",
    "\n",
    "\n",
    "                        elif model == \"ENet\":\n",
    "\n",
    "                            if outcome in binary_outcomes:\n",
    "\n",
    "                                pipe = Pipeline([('processing', preprocessor), \n",
    "                                                 ('estimator', linear_model.LogisticRegression(penalty = 'elasticnet', \n",
    "                                                                                               solver='saga', \n",
    "                                                                                               class_weight = 'balanced',\n",
    "                                                                                               random_state = rep, \n",
    "                                                                                               max_iter = 1000\n",
    "                                                                                              ))])\n",
    "\n",
    "                                model_params = {'estimator__C': Real(1e-2, 1e2, prior = 'log-uniform'),\n",
    "                                               'estimator__l1_ratio': Real(0, 1)\n",
    "                                               }\n",
    "\n",
    "                                cv_model = BayesSearchCV(pipe, \n",
    "                                                        search_spaces = model_params, \n",
    "                                                        scoring='roc_auc', # auc is most important metric\n",
    "                                                        refit=True, \n",
    "                                                        cv=5, n_jobs = 7,\n",
    "                                                        n_iter = 15, \n",
    "                                                        random_state = rep\n",
    "                                                        )\n",
    "\n",
    "                                cv_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "                                y_pred = cv_model.predict(x_test)\n",
    "                                y_prob = cv_model.predict_proba(x_test)\n",
    "                                y_prob_vec = y_prob[:,1]\n",
    "\n",
    "\n",
    "                            elif outcome in continuous_outcomes:\n",
    "\n",
    "                                print(\"In Progress\")\n",
    "\n",
    "\n",
    "\n",
    "                            explainer = shap.LinearExplainer(cv_model.best_estimator_.named_steps['estimator'], x_train_std)\n",
    "                            shap_values = explainer.shap_values(x_test_std)\n",
    "\n",
    "\n",
    "                        elif model == \"XGBoost\":\n",
    "\n",
    "                            if outcome in binary_outcomes:\n",
    "\n",
    "                                weight_vec = wt.compute_sample_weight(class_weight = \"balanced\", y = y_train) # compute weights using sklearn\n",
    "    #                            eval_weight = wt.compute_sample_weight(class_weight = \"balanced\", y = y_val)\n",
    "\n",
    "                                pipe = Pipeline([('processing', preprocessor), \n",
    "                                                 ('estimator', xgb.XGBClassifier(#n_estimators = 500,\n",
    "                                                                                 #eval_metric = \"auc\",\n",
    "                                                                                 use_label_encoder=False,\n",
    "                                                                                 verbosity = 1, random_state = rep\n",
    "                                                                                ))]) \n",
    "\n",
    "\n",
    "                                es = xgb.callback.EarlyStopping(\n",
    "                                    rounds = 10,\n",
    "                                    save_best=True,\n",
    "                                )\n",
    "\n",
    "\n",
    "                                model_params = {\n",
    "                                    'estimator__n_estimators': Integer(50, 250),\n",
    "                                    'estimator__max_depth': Integer(2, 10),\n",
    "                                    'estimator__subsample': Real(0.5, 1.0),\n",
    "                                    'estimator__learning_rate': Real(0.1, 0.5),\n",
    "                                    'estimator__colsample_bytree': Real(0.5, 1.0),\n",
    "                                    'estimator__colsample_bylevel': Real(0.5, 1.0),\n",
    "                                    'estimator__min_child_weight': Integer(1, 10),\n",
    "                                    'estimator__gamma': Real(0, 1.0),\n",
    "                                    'estimator__reg_lambda': Real(1e-2, 1e2, prior = 'log-uniform'),\n",
    "                                    'estimator__reg_alpha': Real(1e-2, 1e2, prior = 'log-uniform'),\n",
    "\n",
    "                                }\n",
    "\n",
    "                                fit_params = {#'estimator__sample_weight_eval_set': [eval_weight],\n",
    "    #                                          'estimator__eval_set': [(x_val_std, y_val)],\n",
    "    #                                          'estimator__callbacks': [es],\n",
    "                                              'estimator__sample_weight': weight_vec,\n",
    "    #                                          'estimator__early_stopping_rounds': 10\n",
    "\n",
    "                                }\n",
    "\n",
    "                                cv_model = BayesSearchCV(pipe, \n",
    "                                                        search_spaces = model_params, n_iter = 15,\n",
    "                                                        scoring='roc_auc', \n",
    "                                                        refit=True, cv= 5, \n",
    "                                                        n_jobs = 7, random_state = rep)\n",
    "\n",
    "                                cv_model.fit(x_train, y_train, **fit_params)\n",
    "\n",
    "\n",
    "\n",
    "                                y_pred = cv_model.predict(x_test)\n",
    "                                y_prob = cv_model.predict_proba(x_test) # generate prediction probability ([0,1])\n",
    "                                y_prob_vec = y_prob[:,1]\n",
    "\n",
    "\n",
    "\n",
    "                            elif outcome in continuous_outcomes:\n",
    "\n",
    "                                print(\"In Progress\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            explainer = shap.TreeExplainer(cv_model.best_estimator_.named_steps['estimator'], data=x_train_std, model_output = \"probability\")\n",
    "                            shap_values = explainer.shap_values(x_test_std)\n",
    "\n",
    "\n",
    "\n",
    "                        elif model == \"RF\":\n",
    "                            \n",
    "\n",
    "                            if outcome in binary_outcomes:\n",
    "\n",
    "                                pipe = Pipeline([('processing', preprocessor), \n",
    "                                                 ('estimator', BalancedRandomForestClassifier(random_state = rep))]) \n",
    "\n",
    "\n",
    "                                model_params = {'estimator__n_estimators': Integer(50, 250),\n",
    "                                               'estimator__max_features': Integer(2, n_features),\n",
    "                                               'estimator__max_depth': Integer(1, 25),\n",
    "                                               'estimator__min_samples_split': Integer(2, 10),\n",
    "                                               'estimator__min_samples_leaf': Integer(1,5),\n",
    "                                               }\n",
    "\n",
    "                                cv_model = BayesSearchCV(pipe,\n",
    "                                                        search_spaces = model_params, n_iter = 15,\n",
    "                                                              scoring='roc_auc', # AUC is most important metric\n",
    "                                                        refit=True, cv=5, n_jobs = 7,\n",
    "                                                        random_state = rep)\n",
    "\n",
    "                                cv_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "                                y_pred = cv_model.predict(x_test) # generate prediction (0 or 1)\n",
    "                                y_prob = cv_model.predict_proba(x_test) # generate prediction probability ([0,1])\n",
    "                                y_prob_vec = y_prob[:,1]\n",
    "\n",
    "                            elif outcome in continuous_outcomes:\n",
    "\n",
    "                                print(\"In Progress\")\n",
    "\n",
    "\n",
    "                            explainer = shap.TreeExplainer(cv_model.best_estimator_.named_steps['estimator'], data=x_train_std, model_output = \"probability\")\n",
    "                            shap_values = explainer.shap_values(x_test_std)[0]\n",
    "\n",
    "\n",
    "                        elif model == \"Dummy\": \n",
    "                            if outcome in binary_outcomes:\n",
    "\n",
    "                                dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "                                dummy_clf.fit(x_train, y_train)\n",
    "\n",
    "                                y_pred = dummy_clf.predict(x_test)\n",
    "                                y_prob = dummy_clf.predict_proba(x_test)\n",
    "                                y_prob_vec = y_prob[:,1]\n",
    "\n",
    "                            elif outcome in continuous_outcomes:\n",
    "\n",
    "                                dummy_clf = DummyRegressor(strategy=\"median\")\n",
    "                                dummy_clf.fit(x_train, y_train)\n",
    "\n",
    "                                y_pred = np.round(dummy_clf.predict(x_test))\n",
    "\n",
    "                        elif model == \"NN\":\n",
    "\n",
    "\n",
    "                            if outcome in binary_outcomes:\n",
    "\n",
    "                                x_train_actual_std = preprocessor.fit_transform(x_train_actual)\n",
    "                                x_val_std = preprocessor.transform(x_val)\n",
    "\n",
    "    #                            x_train_std = preprocessor.fit_transform(x_train)\n",
    "                                x_test_std = preprocessor.transform(x_test)\n",
    "\n",
    "                                reset_random_seed(rep)\n",
    "\n",
    "\n",
    "                                metric_set=[tensorflow.keras.metrics.AUC(name=\"auc\")]\n",
    "\n",
    "\n",
    "\n",
    "                                weight_vec = wt.compute_sample_weight(class_weight = \"balanced\", y = y_train_actual) # compute weights using sklearn\n",
    "                                eval_weight = wt.compute_sample_weight(class_weight = \"balanced\", y = y_val)\n",
    "\n",
    "\n",
    "\n",
    "                                nn_model = models.Sequential(name=\"DeepNN_CLF\", layers=[\n",
    "\n",
    "                                    # Input layer is implicitly defined here\n",
    "\n",
    "                                    ### hidden layer 1\n",
    "                                    layers.Dense(name=\"h1\", input_dim=n_features,\n",
    "                                                 units=int(round((n_features+1)/2)), \n",
    "                                                 activation='relu'),\n",
    "\n",
    "                                    ### dropout layer 1\n",
    "                                    layers.Dropout(name=\"drop1\", rate=0.2, seed = rep),\n",
    "\n",
    "                                    ### hidden layer 2\n",
    "                                    layers.Dense(name=\"h2\", units=int(round((n_features+1)/4)), \n",
    "                                                 activation='relu'),\n",
    "\n",
    "                                    ### dropout layer 2\n",
    "                                    layers.Dropout(name=\"drop2\", rate=0.2, seed = rep),\n",
    "\n",
    "                                    ### layer output\n",
    "                                    layers.Dense(name=\"output\", units=1, activation='sigmoid')\n",
    "                                ])\n",
    "\n",
    "                                nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics = metric_set)\n",
    "\n",
    "                                early_stopping = callbacks.EarlyStopping(\n",
    "                                    monitor='val_auc', \n",
    "                                    verbose=1,\n",
    "                                    patience=5,\n",
    "                                    mode='max',\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "                                nn_model.fit(\n",
    "                                    x_train_actual_std,\n",
    "                                    y_train_actual,\n",
    "                                    sample_weight = weight_vec,\n",
    "                                    epochs= 100,\n",
    "                                    callbacks=[early_stopping],\n",
    "                                    validation_data=(x_val_std, y_val, eval_weight)\n",
    "                                )\n",
    "\n",
    "                                y_pred = (nn_model.predict(x_test_std) > 0.5).astype(\"int32\")                 \n",
    "                                y_prob = nn_model.predict(x_test_std) \n",
    "                                y_prob_vec = y_prob[:,0]\n",
    "\n",
    "                                nn_model.summary()\n",
    "\n",
    "\n",
    "                            elif outcome in continuous_outcomes:\n",
    "\n",
    "                                print(\"In Progress\")\n",
    "\n",
    "\n",
    "\n",
    "                            explainer = shap.DeepExplainer(nn_model, data=x_train_actual_std)\n",
    "                            shap_values = explainer.shap_values(x_test_std)[0]\n",
    "\n",
    "\n",
    "\n",
    "                        # Computing Results regardless of model\n",
    "\n",
    "                        if outcome in binary_outcomes:\n",
    "                            # Computing AUC\n",
    "                            fpr, tpr, threshold = metrics.roc_curve(y_test, y_prob_vec, pos_label = 1)\n",
    "                            roc_auc = metrics.auc(fpr, tpr)\n",
    "                            AUC = metrics.auc(fpr, tpr)\n",
    "\n",
    "                            print(AUC)\n",
    "                            # Sensitivity, Specificity\n",
    "\n",
    "                            recall_score_0 = metrics.recall_score(y_test,y_pred, pos_label=0)\n",
    "                            recall_score_1 = metrics.recall_score(y_test,y_pred, pos_label=1)\n",
    "\n",
    "                            # PPV, NPV\n",
    "                            precision_score_0 = metrics.precision_score(y_test,y_pred, pos_label=0)\n",
    "                            precision_score_1 = metrics.precision_score(y_test,y_pred, pos_label=1)\n",
    "\n",
    "                            # Computing Log Loss\n",
    "                            log_loss = metrics.log_loss(y_test, y_prob_vec, labels = [0,1])\n",
    "\n",
    "                            # Computing Brier score\n",
    "                            brier_loss = metrics.brier_score_loss(y_test, y_prob_vec, pos_label = 1)\n",
    "\n",
    "                            # Computing WF1\n",
    "                            W_F1= metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "                            # Computing Accuracy\n",
    "                            accuracy_score = metrics.accuracy_score(y_test,y_pred)\n",
    "                            B_accuracy_score = metrics.balanced_accuracy_score(y_test,y_pred)\n",
    "\n",
    "\n",
    "                            # Confusion Matrix\n",
    "                            c_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "                            new_results = pd.DataFrame(\n",
    "                                {\n",
    "                                    \"Procedure\": [procedure],\n",
    "                                    \"Outcome\": [outcome],\n",
    "                                    \"Features\": [feature],\n",
    "                                    \"Model\": [model],\n",
    "                                    \"Rep\": [rep],\n",
    "                                    \"Fold\": [i],\n",
    "                                    \"W_F1\": [W_F1],\n",
    "                                    \"AUC\": [AUC],\n",
    "                                    \"MAE\": [\"NA\"],\n",
    "                                    \"R2\": [\"NA\"],\n",
    "                                    \"RMSE\": [\"NA\"],\n",
    "                                    \"Log-Loss\": [log_loss],\n",
    "                                    \"Brier-Loss\": [brier_loss],\n",
    "                                    \"Accuracy\": [accuracy_score],\n",
    "                                    \"Balanced_Accuracy\": [B_accuracy_score],\n",
    "                                    \"Sensitivity\": [recall_score_1],\n",
    "                                    \"Specificity\": [recall_score_0],\n",
    "                                    \"NPV\": [precision_score_0],\n",
    "                                    \"PPV\": [precision_score_1],\n",
    "                                    \"c_matrix\": [c_mat],\n",
    "                                    \"n\":[n],\n",
    "                                    \"label_n\": [label_n]\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                            auc_plot_results_new = pd.DataFrame(\n",
    "                                {\n",
    "                                    \"Procedure\": np.repeat(procedure, len(fpr)),\n",
    "                                    \"Outcome\": np.repeat(outcome, len(fpr)),\n",
    "                                    \"Features\": np.repeat(feature, len(fpr)),\n",
    "                                    \"Model\": np.repeat(model, len(fpr)),\n",
    "                                    \"Rep\": np.repeat(rep, len(fpr)),\n",
    "                                    \"Fold\": np.repeat(i, len(fpr)),\n",
    "                                    \"FPR\": fpr,\n",
    "                                    \"TPR\": tpr,\n",
    "                                    \"Thresholds\": threshold\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                            auc_plot_results.append(auc_plot_results_new)\n",
    "\n",
    "\n",
    "                        elif outcome in continuous_outcomes:\n",
    "                            print(\"In Progress\")\n",
    "\n",
    "\n",
    "                        all_results.append(new_results)\n",
    "\n",
    "                        if model == \"Dummy\":\n",
    "                            continue\n",
    "\n",
    "                        if model == \"ASA\" or model == \"ASA_categorical\": \n",
    "                            continue\n",
    "\n",
    "                        mod_shap = shap_values.copy()\n",
    "                        #mod_shap = shap_values[0]\n",
    "\n",
    "                        test_length = len(y_test)\n",
    "\n",
    "                        row_vec = list(range(0, test_length))\n",
    "                        shap_features_row = []\n",
    "\n",
    "                        for row in row_vec:\n",
    "\n",
    "                            shap_values_row = mod_shap[row]\n",
    "\n",
    "                            temp_feature_names = list(x_cols)\n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "                            new_shap_features_row = pd.DataFrame(\n",
    "                                {\n",
    "                                    \"Patient_ID\": np.repeat(row, len(x_cols)),\n",
    "                                    \"Feature_Name\": temp_feature_names,\n",
    "                                    \"Feature_Value\": shap_values_row,\n",
    "                                    \"Feature_Abs_Value\": np.abs(shap_values_row),\n",
    "\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                            shap_features_row.append(new_shap_features_row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        shap_features_tabular = pd.concat(shap_features_row)\n",
    "\n",
    "\n",
    "\n",
    "                        individual_features_shap = x_test_features_long.merge(shap_features_tabular, how = \"left\", on = [\"Patient_ID\", \"Feature_Name\"])\n",
    "\n",
    "                        individual_features_shap[\"Procedure\"]=  procedure\n",
    "                        individual_features_shap[\"Outcome\"]=  outcome\n",
    "                        individual_features_shap[\"Model\"]=  model\n",
    "                        individual_features_shap[\"Rep\"]=  rep\n",
    "                        individual_features_shap[\"Fold\"]=  i\n",
    "\n",
    "\n",
    "                        individual_features_shap_full.append(individual_features_shap)\n",
    "\n",
    "                        shap_file_results = pd.concat(individual_features_shap_full, ignore_index = True)\n",
    "                        shap_file_results.to_csv(shap_file_name)  \n",
    "\n",
    "\n",
    "                        summarized_features_shap = shap_features_tabular.groupby(['Feature_Name']).agg(   \n",
    "                            Feature_Mean_Abs = ('Feature_Abs_Value','mean'),\n",
    "                        ).reset_index()\n",
    "\n",
    "\n",
    "                        feature_coef = summarized_features_shap['Feature_Mean_Abs'].values\n",
    "                        feature_names = summarized_features_shap['Feature_Name'].values\n",
    "\n",
    "\n",
    "                        new_features = pd.DataFrame(\n",
    "                            {\n",
    "                                \"Procedure\": np.repeat(procedure, len(feature_names)),\n",
    "                                \"Outcome\": np.repeat(outcome, len(feature_names)),\n",
    "                                \"Features\": np.repeat(feature, len(feature_names)),\n",
    "                                \"Model\": np.repeat(model, len(feature_names)),\n",
    "                                \"Rep\": np.repeat(rep, len(feature_names)),\n",
    "                                \"Fold\": np.repeat(i, len(feature_names)),\n",
    "                                \"Feature_Name\": feature_names,\n",
    "                                \"Feature_Mean_Abs_Value\": feature_coef,\n",
    "\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "\n",
    "    #                    feature_results = feature_results.append(new_features, ignore_index=True)\n",
    "                        feature_results.append(new_features)\n",
    "\n",
    "\n",
    "\n",
    "    #                    all_results = all_results.append(new_results, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    file_name_base = procedure+\"_\"+date+\"_\"\n",
    "    \n",
    "    output_file_results = pd.concat(all_results, ignore_index = True)\n",
    "    output_file_results = output_file_results[output_file_results['Procedure'] == procedure]\n",
    "    output_file_results.to_csv(file_name_base+\"_\"+\"model_results.csv\")\n",
    "    \n",
    "    \n",
    "    feature_file_results = pd.concat(feature_results, ignore_index = True)\n",
    "    feature_file_results = feature_file_results[feature_file_results['Procedure'] == procedure]\n",
    "    feature_file_results.to_csv(file_name_base+\"_\"+\"feature_importance.csv\")    \n",
    "    \n",
    "    \n",
    "    #print(auc_plot_results)\n",
    "    auc_file_results = pd.concat(auc_plot_results, ignore_index = True)\n",
    "    auc_file_results = auc_file_results[auc_file_results['Procedure'] == procedure]\n",
    "    auc_file_results.to_csv(file_name_base+\"_\"+\"auc_data.csv\") \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(time.strftime('%X %x %Z'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results_output = pd.concat(feature_results, ignore_index = True)\n",
    "feature_results_output.to_csv(\"thumb_arthro_features_3_11_23_partial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(np.repeat(row, len(x_cols)))\n",
    "print(len(shap_values_row))\n",
    "print(len(temp_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_features_shap.to_csv(\"tester_shap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    output_file_results.to_csv(file_name_results)\n",
    "    \n",
    "    \n",
    "    feature_file_results = pd.concat(feature_results)\n",
    "    feature_file_results = feature_file_results[feature_file_results['Procedure'] == procedure]\n",
    "    feature_file_results.to_csv(file_name_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_plot_results.to_csv(\"auc_results_health_util_11_7_22.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(output_file_results)\n",
    "np.median(output_file_results['MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_features_row)\n",
    "\n",
    "#shap_features_row.to_csv(\"NN_feature_test.csv\")\n",
    "\n",
    "summarized_features_shap = shap_features_row.groupby(['Feature_Name']).agg(\n",
    "    Feature_Mean_Abs = ('Feature_Value_Abs','mean'),   \n",
    "    Feature_Mean_Real = ('Feature_Value_Real','mean') \n",
    ").reset_index()\n",
    "\n",
    "print(summarized_features_shap)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install joblib==1.1.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(feature_results)\n",
    "#print(all_results)\n",
    "#feature_results.to_csv(\"feature_test_1_30_21.csv\")\n",
    "partial_output_results = pd.concat(all_results)\n",
    "partial_output_features = pd.concat(feature_results)\n",
    "partial_output_results.to_csv(\"partial_acdf_results_5_31_22.csv\")\n",
    "partial_output_features.to_csv(\"partial_acdf_features_5_31_22.csv\")\n",
    "#hyperparam_results.to_csv(\"all_results_acdf_cbc_bmp_hyperparam_lasso_brf_xgb_2_6_22_updated.csv\")\n",
    "\n",
    "print(feature_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_output_file_results = pd.concat(all_results)\n",
    "partial_output_file_results = partial_output_file_results[partial_output_file_results['Procedure'] == \"PLF\"]\n",
    "partial_output_file_results.to_csv(\"partial_plf_results_5_31_22.csv\")\n",
    "    \n",
    "    \n",
    "partial_feature_file_results = pd.concat(feature_results)\n",
    "partial_feature_file_results = partial_feature_file_results[partial_feature_file_results['Procedure'] == \"PLF\"]\n",
    "partial_feature_file_results.to_csv(\"partial_plf_features_5_31_22.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outcome)\n",
    "print(procedure)\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_results = all_results.groupby(['Outcome', 'Features', 'Model']).agg(\n",
    "    WF1_Mean = ('W_F1','mean'),\n",
    "    WF1_SD = ('W_F1', 'std'),\n",
    "    AUC_Mean = ('AUC', 'mean'),\n",
    "    AUC_SD = ('AUC', 'std'),\n",
    "    Acc_Mean = ('Accuracy', 'mean'),\n",
    "    Acc_SD = ('Accuracy', 'std'),\n",
    "    BAcc_Mean = ('Balanced_Accuracy', 'mean'),\n",
    "    BAcc_SD = ('Balanced_Accuracy', 'std'),\n",
    "    Log_Loss_Mean = ('Log-Loss', 'mean'),\n",
    "    Log_Loss_SD = ('Log-Loss', 'std'),\n",
    "    Precision_Neg_Mean = ('Precision_Neg', 'mean'),\n",
    "    Precision_Neg_SD = ('Precision_Neg', 'std'),\n",
    "    Recall_Neg_Mean = ('Recall_Neg', 'mean'),\n",
    "    Recall_Neg_SD = ('Recall_Neg', 'std'),\n",
    "    Precision_Pos_Mean = ('Precision_Pos', 'mean'),\n",
    "    Precision_Pos_SD = ('Precision_Pos', 'std'),\n",
    "    Recall_Pos_Mean = ('Recall_Pos', 'mean'),\n",
    "    Recall_Pos_SD = ('Recall_Pos', 'std'),\n",
    "    n_Mean = ('n', 'mean'),\n",
    "    n_SD = ('n', 'std'),\n",
    "    label_n_Mean = ('label_n', 'mean'),\n",
    "    label_n_SD = ('label_n', 'std')\n",
    "    \n",
    "    \n",
    ").reset_index()\n",
    "\n",
    "summarized_results.to_csv(\"wnd_dehis_summarized_lasso_brf_xgb_2_6_22_updated.csv\")\n",
    "\n",
    "print(summarized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_coef)\n",
    "print(feature_coef_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_dat = pd.read_csv(r\"Results/LASSO/pre_op_features_2_1_22.csv\", sep=',')\n",
    "feature_dat = pd.read_csv(r\"wnd_dehis_features_lasso_brf_xgb_2_3_22_updated.csv\", sep=',')\n",
    "\n",
    "\n",
    "feature_dat['Feature_Value_Real'] = feature_dat['Feature_Value_Real'].str.strip('[]').astype(float) # uncomment and run this with lasso\n",
    "\n",
    "summarized_features = feature_dat.groupby(['Outcome', 'Model', 'Feature_Name']).agg(\n",
    "    Feature_Value_Mean = ('Feature_Value_Real', 'mean'),\n",
    "    Feature_Value_SD = ('Feature_Value_Real', 'std'),\n",
    "    \n",
    ").reset_index()\n",
    "\n",
    "summarized_features.to_csv(\"wnd_dehiscence_xgb_features_summarized_2_3_22_updated.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
